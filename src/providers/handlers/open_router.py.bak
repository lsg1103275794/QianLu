"""
OpenRouter API handler implementation (OpenAI Compatible).
OpenRouter acts as a proxy/aggregator for many models.
"""
import json
import aiohttp
from typing import Optional, Dict, Any, List, AsyncGenerator
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type, RetryError

from src.utils.logging import logger
from src.providers.base import BaseAPIHandler
from src.utils.error_handler import ConfigError, APIError, APIResponseError
from src.utils.retry import is_retryable_exception

class OpenRouterHandler(BaseAPIHandler):
    """Handles interaction with OpenRouter API (OpenAI Compatible)."""

    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.provider_name = config.get('provider_name', 'open_router')
        self.api_key = config.get('api_key')
        self.endpoint = config.get('endpoint', 'https://openrouter.ai/api/v1') # Default OpenRouter endpoint
        # OpenRouter model names are usually prefixed, e.g., 'mistralai/mistral-7b-instruct'
        self.default_model = config.get('default_model', 'mistralai/mistral-7b-instruct') 
        self.default_api_params = {
            'temperature': config.get('temperature', 0.7),
            'top_p': config.get('top_p', 1.0),
            'max_tokens': config.get('max_tokens', 2048)
            # OpenRouter specific: route, transforms (passed via kwargs)
        }
        # Custom HTTP headers for OpenRouter (optional)
        self.http_referer = config.get('http_referer', None) # Your site URL
        self.x_title = config.get('x_title', None) # Your app name

        if not self.api_key:
            raise ConfigError(f"Provider '{self.provider_name}' is missing required 'api_key' configuration.")
        if not self.endpoint:
            raise ConfigError(f"Provider '{self.provider_name}' is missing required 'endpoint' configuration.")

        # Ensure endpoints are set correctly
        if self.endpoint.endswith('/'):
            self.chat_endpoint = f"{self.endpoint}chat/completions"
            self.models_endpoint = f"{self.endpoint}models"
        else:
            self.chat_endpoint = f"{self.endpoint}/chat/completions"
            self.models_endpoint = f"{self.endpoint}/models"

        logger.info(f"OpenRouter Handler Initialized: Name='{self.provider_name}', Endpoint='{self.endpoint}', DefaultModel='{self.default_model}'")

    def get_required_config_fields(self) -> List[str]:
        return ['api_key'] 

    def _prepare_headers(self) -> Dict[str, str]:
        """Prepare standard headers including OpenRouter specific ones."""
        headers = {
            'Content-Type': 'application/json',
            'Authorization': f'Bearer {self.api_key}'
        }
        if self.http_referer:
            headers['HTTP-Referer'] = self.http_referer
        if self.x_title:
            headers['X-Title'] = self.x_title
        return headers

    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=2, max=10),
        retry=retry_if_exception_type(APIError),
        reraise=True
    )
    async def get_available_models(self) -> List[str]:
        # Standard OpenAI-compatible model fetching
        try:
            headers = self._prepare_headers()
            async with aiohttp.ClientSession() as session:
                async with session.get(self.models_endpoint, headers=headers, timeout=20) as response: # Increased timeout for potentially large list
                    if response.status != 200:
                        error_text = await response.text()
                        logger.error(f"OpenRouter API error fetching models: {response.status}, {error_text}")
                        raise APIError(f"Failed to fetch models: HTTP {response.status}")
                    
                    data = await response.json()
                    models = [model['id'] for model in data.get('data', []) if isinstance(model, dict) and 'id' in model]
                    logger.info(f"Available OpenRouter models: {len(models)}")
                    return models
        except aiohttp.ClientError as e:
            logger.error(f"OpenRouter API connection error fetching models: {e}")
            raise APIError(f"Connection error: {e}")
        except Exception as e:
            logger.error(f"Error getting OpenRouter models: {str(e)}")
            return [self.default_model] if self.default_model else []

    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=2, max=10),
        retry=retry_if_exception_type((APIError, APIResponseError)),
        reraise=True
    )
    async def _make_request(self, headers: Dict[str, str], payload: Dict[str, Any]) -> Dict[str, Any]:
        logger.debug(f"Sending request to OpenRouter endpoint: {self.chat_endpoint}")
        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    self.chat_endpoint,
                    headers=headers,
                    json=payload,
                    timeout=60
                ) as response:
                    if response.status != 200:
                        error_text = await response.text()
                        logger.error(f"OpenRouter API HTTP error: status={response.status}, response: '{error_text[:200]}...'")
                        raise APIResponseError(f"HTTP {response.status}: {error_text}")
                    return await response.json()
        except aiohttp.ClientError as e:
            logger.error(f"OpenRouter API request failed (network error): {e}")
            raise APIError(str(e))
        except json.JSONDecodeError as e:
            logger.error(f"Failed to decode JSON response from OpenRouter: {e}")
            raise APIResponseError(f"Invalid JSON response: {e}")

    async def generate(
        self,
        prompt: str,
        model: Optional[str] = None,
        **kwargs
    ) -> str:
        target_model = model or self.default_model
        if not target_model:
            raise ConfigError("No model specified and no default model configured for OpenRouter")

        messages = [{"role": "user", "content": prompt}]
        final_api_params = self.default_api_params.copy()
        final_api_params.update(kwargs)
        
        payload = {
            "model": target_model,
            "messages": messages,
            "stream": False, 
            **final_api_params
            # OpenRouter specific params like 'route' can be passed in kwargs
        }

        headers = self._prepare_headers()

        try:
            result = await self._make_request(headers=headers, payload=payload)
            if "choices" in result and isinstance(result["choices"], list) and len(result["choices"]) > 0:
                first_choice = result["choices"][0]
                if "message" in first_choice and "content" in first_choice["message"]:
                    return first_choice["message"]["content"].strip()
            logger.warning(f"OpenRouter response format unexpected: {result}")
            raise APIResponseError("Unexpected response format from OpenRouter")
        except RetryError as e:
            logger.critical(f"OpenRouter API request failed after retries: {e}")
            raise APIError(f"API request failed after retries: {e}")

    async def stream_chat(
        self,
        messages: List[Dict[str, Any]],
        model: Optional[str] = None,
        **kwargs
    ) -> AsyncGenerator[Dict[str, Any], None]:
        target_model = model or self.default_model
        if not target_model:
            raise ConfigError("No model specified and no default model configured for OpenRouter")

        final_api_params = self.default_api_params.copy()
        final_api_params.update(kwargs)

        payload = {
            "model": target_model,
            "messages": messages,
            "stream": True,
            **final_api_params
        }

        headers = self._prepare_headers()

        try:
            yield {"choices": [{"delta": {"role": "assistant"}}]}
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    self.chat_endpoint,
                    headers=headers,
                    json=payload,
                    timeout=60
                ) as response:
                    if response.status != 200:
                        error_text = await response.text()
                        logger.error(f"OpenRouter Stream API error: {response.status}, {error_text}")
                        yield {"error": f"API error: HTTP {response.status}"}
                        return

                    async for line in response.content:
                        if not line:
                            continue
                        line_str = line.decode('utf-8').strip()
                        if not line_str:
                            continue
                        if line_str.startswith("data: "):
                            data_str = line_str[6:].strip()
                            if data_str == "[DONE]":
                                logger.info("OpenRouter Stream chat completed")
                                break
                            try:
                                data = json.loads(data_str)
                                yield data 
                            except json.JSONDecodeError as e:
                                logger.error(f"Failed to parse OpenRouter stream response: {e}, data: '{data_str}'")
        except Exception as e:
            logger.error(f"OpenRouter stream chat error: {str(e)}")
            yield {"error": str(e)} 